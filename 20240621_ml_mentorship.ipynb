{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/noe2001/Mybetterworld/blob/master/20240621_ml_mentorship.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What MUST you (as an organization and as an individual) have to succeed with AI/ML?"
      ],
      "metadata": {
        "id": "Fd5HJZ2UtS4j"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pHbJKuwtoH6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MDqRZKGHoILN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Types of Machine Learning\n",
        "\n",
        "## Supervised\n",
        "\n",
        "Use labeled data to train models to predict some outcomes. The classic, but it can take a lot of time to produce a labeled dataset.\n",
        "\n",
        "**Example**: Predicting if a unit will pass inspection.\n",
        "\n",
        "**Tip**: Sometimes labels can be efficiently generated with hindsight. For example, a product failing inspection can be recorded and then used to build a dataset with values from when the product was being manufactured.\n",
        "\n",
        "## Unsupervised\n",
        "\n",
        "Analyze unlabeled data to detect patterns imperceptible to the naked eye. Can be difficult to evaluate in practice.\n",
        "\n",
        "**Example**: Grouping similar batches using a nearest-neighbours approach for various measurements taken during that each batch's creation.\n",
        "\n",
        "## Semi-Supervised\n",
        "\n",
        "A combination of the previous two methods: we augment a small, valuable collection of labeled data with a larger, cheaper set of unlabeled data.\n",
        "\n",
        "**Example**: Given a small collection of labeled machine statuses (correct vs faulty), augment the data with a large amount of unlabeled machine status to improve machine status classification.\n",
        "\n",
        "## A Note: Regression vs. Classification\n",
        "\n",
        "**Regression** is predicting continuous values as a function of the inputs, whereas **classification** is concerned with predicting a group or \"class\" that an input belongs to. Many types of ML models can be adapted to either task.\n",
        "\n",
        "## A Note: Reinforcement Learning\n",
        "\n",
        "A class of ML where an agent learns to act in an environment via trial and error, imitation, or other techniques."
      ],
      "metadata": {
        "id": "h_30k_LEvnqC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Making a Model\n",
        "\n",
        "Machine learning and AI are tools to leverage data and domain knowledge to produce actionable information or automatically act within a system.\n"
      ],
      "metadata": {
        "id": "Bx6quY15PQx7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Machine Learning Libraries\n",
        "\n",
        "- `numpy`: A performant mathematical library written in C.\n",
        "- `pandas`: A library for creating and manipulating tables of data called dataframes.\n",
        "- `scikit-learn` (import as `sklearn`): A library with a large array of data science and machine learning tools.\n",
        "- `torch`, `tensorflow`: Graph computation libraries for neural networks. Can exploit GPUs to train large models efficiently."
      ],
      "metadata": {
        "id": "W_IIBCsa8MVu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajryLY2DNt5R"
      },
      "outputs": [],
      "source": [
        "# A mathematical library, written in C for speed, that works on N-dimensional arrays\n",
        "import numpy as np\n",
        "\n",
        "# A library for creating and manipulating tables of data\n",
        "import pandas as pd\n",
        "\n",
        "# A library that implements various AI/ML and data science algorithms\n",
        "# Usually not imported outright\n",
        "# from sklearn import ...\n",
        "\n",
        "# Libraries for neural networks that use computational graphs\n",
        "# for efficient processing, and can also use the GPU!\n",
        "# import tensorflow as tf\n",
        "import torch\n",
        "torch.cuda.is_available(), torch.cuda.device_count()\n",
        "# Also mps backend for macs!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can set random seeds for reproducibility in teaching. Generally, you won't do this in practice."
      ],
      "metadata": {
        "id": "GwfsQ_1Yjtxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(20240621)\n",
        "torch.manual_seed(20240621)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(20240621)"
      ],
      "metadata": {
        "id": "h8sdCJBYiO62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll want this later, so we can install it now."
      ],
      "metadata": {
        "id": "x9NkaixmJLjc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchviz"
      ],
      "metadata": {
        "id": "nHA_GdH7JK-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Identify a opportunity\n",
        "\n",
        "Seek out areas in your business where you have data that is not being fully leveraged. Find something that would be valuable to know in advance. Understand how and why failures happen in production, even when there's no pattern obvious to the naked eye."
      ],
      "metadata": {
        "id": "Mgv7HKBqM0ed"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gather a dataset\n",
        "\n",
        "Given a source of data and one or more problems to solve relating to that data,\n",
        "can we:\n",
        "- Solve or model those problems using AI/ML?\n",
        "- Use ML to capture patterns or trends in the data relating to those problems?\n",
        "\n",
        "We'll just generate some sample data, but this could be connecting to a historian, loading image files, or any other data input."
      ],
      "metadata": {
        "id": "e8hnbdV9bnft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "\n",
        "raw_x, raw_y = make_classification(\n",
        "    n_samples=500,\n",
        "    n_features=10, n_redundant=1, n_repeated=1, n_informative=8,\n",
        "    random_state=20240621\n",
        ")\n",
        "feature_names = [f'x{i}' for i in range(raw_x.shape[1])]\n",
        "df = pd.DataFrame(raw_x, columns=feature_names)\n",
        "df['y'] = raw_y\n",
        "df.head()"
      ],
      "metadata": {
        "id": "HNURd5bCbmtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That was easy - but in practice, this can be one of the most labor-intensive steps. Sometimes, the labeling can take more work than everything else combined."
      ],
      "metadata": {
        "id": "Bg4p30WrvZ8r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploratory Data Analysis\n",
        "\n",
        "This could be a dozen lectures in itself, but we'll sum it up as follows:\n",
        "\n",
        "Exploratory data analysis is the process of gaining a general understanding of your dataset.\n",
        "- Data types\n",
        "  - Categorical: machine states, part types\n",
        "  - Discrete: counts, levels\n",
        "  - Continuous: measurements, rates\n",
        "- Data quality\n",
        "  - Missing values?\n",
        "  - Inconsistent formats? (Common for human-entered data!)\n",
        "- Data properties\n",
        "  - Correlations\n",
        "  - Anomalies\n",
        "  - Outliers"
      ],
      "metadata": {
        "id": "moX4s-foo7sr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "fimpO-wQhp8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's always useful to know how many of each class we have in our training data."
      ],
      "metadata": {
        "id": "gKyZXL6bclcg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"y\"].value_counts()"
      ],
      "metadata": {
        "id": "2ATA44MujXFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can examine correlation between pairs of variables easily."
      ],
      "metadata": {
        "id": "4OYd9Kf4crQu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A powerful but quick to use plotting library\n",
        "import matplotlib.pyplot as plt\n",
        "# Seaborn is a matplotlib wrapper that has a lot of handy-dandy features for data science!\n",
        "import seaborn as sns\n",
        "\n",
        "# Pairplot can be informative, but it can take a while to run!\n",
        "# sns.pairplot(df, hue=\"target\")\n",
        "\n",
        "correlation_matrix = df.corr()\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')"
      ],
      "metadata": {
        "id": "mNQ_LwYhpef-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(x=\"x1\", y=\"x5\", hue=\"y\", data=df)"
      ],
      "metadata": {
        "id": "dCt6Wt-5psLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tip:** some large language models, like ChatGPT, have features useful for exploratory data analysis!"
      ],
      "metadata": {
        "id": "DX-xE7t0y4eA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Transformations and Feature Engineering\n",
        "\n",
        "Now that we understand our dataset, we can transform it to make it more useful for our models. Often, the domain experts will have practical knowledge as to some useful transformations or derived features!\n",
        "\n",
        "- Converting continuous variables to discrete or categorical\n",
        "  - Categorical bins: Maybe we don't need to worry about a machine's precise temperature, and can just split it into low/medium/high/critical ranges\n",
        "  - Binarization: Maybe a rate can be converted into active/inactive\n",
        "- More complex derived features can extracted\n",
        "  - Temporal features: What day of the week was a given date?\n",
        "  - Total related features: 4 machines each producing something - what is their total output?\n",
        "  - Mathematical transformations: Logariths, roots, polynomials, and so much more!"
      ],
      "metadata": {
        "id": "-m2VygoGsdxO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_engineered = df.copy()\n",
        "df_engineered[\"e1\"] = df_engineered[[\"x1\", \"x8\"]].sum(axis=1)\n",
        "correlation_matrix = df_engineered.corr()\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')"
      ],
      "metadata": {
        "id": "xXof1nhJredh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing the data for modeling\n",
        "\n",
        "For supervised and semi-supervised machine learning, we'll want training and testing data.\n",
        "\n",
        "In practice, there are lots of way to handle this - look up **cross validation** to learn about a powerful way to get more bang for your buck from your data."
      ],
      "metadata": {
        "id": "-Bxcw2_fzz-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(df_engineered.drop(\"y\", axis=1), df_engineered[\"y\"], test_size=0.2, random_state=20240621)"
      ],
      "metadata": {
        "id": "-s7e90R0u7uy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pitfalls\n",
        "\n",
        "- Much manufacturing data is **temporal**. We can accidentally \"cheat\" by putting data from one moment in the training set and the next moment in the test set, when the two moments are almost the same - we've essentially trained using test data!\n",
        "- Make sure you have diverse scenarios captured in both your training and test sets!\n",
        "- **Class imbalance** is when one class is rarer than another. Some modeling techniques and metrics aren't resilient to this.\n",
        "  - If 1% of my products fail QA, a prediction that every part will pass QA is 99% accurrate but 0% useful!"
      ],
      "metadata": {
        "id": "K1BsilBX0NXZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First contact: basic models\n",
        "\n",
        "When possible, I like to start with some basic `scikit-learn` models - they are surprisingly powerful, and sufficient for many tasks! It's tempting to use deep learning or neural networks for everything, but not very practical and often times less effective (large, super-powerful models require a lot of data to effectively train)."
      ],
      "metadata": {
        "id": "7vgTsVcP5V-7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# We limit max depth to prevent tree from just exactly learning the training data!\n",
        "tree1 = DecisionTreeClassifier(max_depth=3, random_state=20240621)\n",
        "tree1.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "48hPKdYi5Z-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize the tree we learned. Trees are explainable models - what kind of value could be gained by analyzing an explainable model?"
      ],
      "metadata": {
        "id": "3Rh3lGCoGff3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import plot_tree\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "_ = plot_tree(tree1, feature_names=X_train.columns)"
      ],
      "metadata": {
        "id": "dtH5n8Re7Rms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the tree to predict labels for our testing data."
      ],
      "metadata": {
        "id": "qLrWYiXOGoHk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = tree1.predict(X_test)\n",
        "y_pred"
      ],
      "metadata": {
        "id": "AYTmnCsp6ooF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Evaluation\n",
        "\n",
        "There are [a lot](https://scikit-learn.org/stable/modules/model_evaluation.html) of possible ways to evaluate a model's performance. Some tasks may require very specialized ways to score.\n",
        "\n",
        "Here, we have balanced classes for a basic binary prediction task, so our usual suspects should suffice."
      ],
      "metadata": {
        "id": "Z1gjU10T6wQ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "print(metrics.classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "jCEqmEEt6vbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The confusion matrix is valuable to know what *kind* of errors our model is making."
      ],
      "metadata": {
        "id": "8nX_eYoFcLl4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics.ConfusionMatrixDisplay(metrics.confusion_matrix(y_test, y_pred), display_labels=tree1.classes_).plot()"
      ],
      "metadata": {
        "id": "NU1xWjQWTd4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not great, but that's okay - it's just a first experiment!"
      ],
      "metadata": {
        "id": "V4najMRu8vKq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Other Models\n",
        "\n",
        "We can try other models (scikit-learn has [a lot of supervised learning techniques](https://scikit-learn.org/stable/supervised_learning.html) available), or we could perform a [hyperparameter search](https://scikit-learn.org/stable/modules/grid_search.html). (Or both! Experimental mindset!)"
      ],
      "metadata": {
        "id": "UnUV4UoF89qO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SVMs are a versatile class of supervised models that can be used for\n",
        "# classification and regression!\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "svm1 = SVC(random_state=20240621)\n",
        "svm1.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "fWsVoceN7MEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's run our metrics again."
      ],
      "metadata": {
        "id": "Gnft8RH2cILF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = svm1.predict(X_test)\n",
        "print(metrics.classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "Z7_sbq4VHKEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics.ConfusionMatrixDisplay(metrics.confusion_matrix(y_test, y_pred), display_labels=tree1.classes_).plot()"
      ],
      "metadata": {
        "id": "-nXf4o0mUVJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Better!"
      ],
      "metadata": {
        "id": "xIrVF2Sr9wEC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Networks\n",
        "\n",
        "Neural networks are class of model that, at their simplest consist of graphs arranged into layers with weighted edges between layers, where the edge weights are learnable. A node's value is multiplied by the weight of its outgoing edges and propagated through the network to the end. Of course, in practice, there are many more details and modifications, but those are beyond the scope of this lecture.\n",
        "\n",
        "Often, neural networks are overkill, but in **computer vision** and **natural language**, they are consistently the most effective tool in the ML scientist's toolkit.\n",
        "\n",
        "Neural networks tend to require a lot of data, and can be efficiently trained on GPUs. We'll just examine a small, toy neural network that we can train quickly."
      ],
      "metadata": {
        "id": "s39lW74eAQ7u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Rescaling"
      ],
      "metadata": {
        "id": "_hrP7pkSHWHK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Neural networks are sensitive to the scale of their inputs\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "9WXbAnZqAQCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Handling for Neural Network Libraries\n",
        "\n",
        "Torch uses DataSets and DataLoaders to manage inputs to its models. There's a bunch of different kinds of each - read the documentation to find the best fit for your application."
      ],
      "metadata": {
        "id": "By3DzbZojT0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A tensor is just a multi-dimensional array\n",
        "# A generalization of scalars, vectors, and matrices\n",
        "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
        "\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
        "\n",
        "# A dataset is used to load and preprocess data.\n",
        "train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = torch.utils.data.TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "# The dataloaders handle iteration, batching, shuffling, and parallelization\n",
        "batch_size = 4\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "X_train_tensor"
      ],
      "metadata": {
        "id": "bJkAt6EojSJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define our own structure for a neural network. In torch, we'll typically do this by extending `nn.Module`."
      ],
      "metadata": {
        "id": "xuM2HXOTFska"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(X_train_tensor.shape[1], 10),\n",
        "            nn.ReLU(), # A ReLU is a type of activation functions\n",
        "            nn.Linear(10, 10),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(10, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "D5qvwwQWFrSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can visualize how the model is structured with the torchviz library we installed earlier."
      ],
      "metadata": {
        "id": "U2sZJ25wcCRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchviz import make_dot\n",
        "\n",
        "dummy_nn = SimpleNN()\n",
        "make_dot(dummy_nn(torch.randn(1, X_train_tensor.shape[1])), params=dict(list(dummy_nn.named_parameters())))"
      ],
      "metadata": {
        "id": "uwNaKk8TH4pz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(271828)\n",
        "\n",
        "nn1 = SimpleNN()\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(nn1.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 100\n",
        "interval = 5\n",
        "losses, val_losses = [], []\n",
        "for epoch in range(1, num_epochs+1):\n",
        "    nn1.train()\n",
        "    total_loss, n_batches = 0, 0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        y_batch = y_batch.unsqueeze(1)\n",
        "\n",
        "        # make a prediction for this value\n",
        "        out = nn1(X_batch)\n",
        "        # find out how wrong we are\n",
        "        loss = criterion(out, y_batch)\n",
        "\n",
        "        # resets the gradients\n",
        "        optimizer.zero_grad()\n",
        "        # computes the new gradients for this pass by working backwards\n",
        "        loss.backward()\n",
        "        # updates the weights with the new gradients\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        n_batches += 1\n",
        "\n",
        "    with torch.no_grad():\n",
        "        nn1.eval()\n",
        "        total_val_loss, n_val_batches = 0, 0\n",
        "        for X_batch, y_batch in test_loader:\n",
        "            y_batch = y_batch.unsqueeze(1)\n",
        "            out = nn1(X_batch)\n",
        "            loss = criterion(out, y_batch)\n",
        "            total_val_loss += loss.item()\n",
        "            n_val_batches += 1\n",
        "\n",
        "    avg_val_loss = total_val_loss/n_val_batches\n",
        "    avg_loss = total_loss/n_batches\n",
        "    losses.append(avg_loss)\n",
        "    val_losses.append(avg_val_loss)\n",
        "\n",
        "    if epoch % interval == 0:\n",
        "        print(f\"Epoch {epoch}/{num_epochs}, Average Train Loss: {avg_loss:.4f}, Average Val Loss: {avg_val_loss:.4f}\")"
      ],
      "metadata": {
        "id": "dndNT3PWIfBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Watch out for overfitting!"
      ],
      "metadata": {
        "id": "-7FocHkjkKH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_loss = pd.DataFrame({\"loss\": losses, \"val_loss\": val_losses})\n",
        "df_loss.plot()"
      ],
      "metadata": {
        "id": "d0kUifKShacy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A Note: Transfer Learning\n",
        "\n",
        "Transfer learning is the use of a pretrained model to speed up training. Transfer learning is incredibly powerful, and can massively reduce the time and data required to train a new model.\n",
        "\n",
        "An example of transfer learning you can play with today is [AWS Rekognition Custom Labels](https://aws.amazon.com/rekognition/custom-labels-features/), which can learn from just a handful of images."
      ],
      "metadata": {
        "id": "Qj5T3nGtFxLm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How do I deploy it? How do I maintain it?\n",
        "\n",
        "Unfortunately, there's no clean answer - every architecture and need is different. Additionally, as the world evolves, your model might **drift** - cease to be accurate to the new reality.\n",
        "\n",
        "Look in to https://madewithml.com/#course for a detailed course on machine learning for production. There are a lot of great courses at https://www.deeplearning.ai/courses/ and https://fullstackdeeplearning.com/.\n",
        "\n",
        "Remember, your goal should be to reach the skill level needed for your interests or tasks, then to always improve your skills and knowledge bit by bit."
      ],
      "metadata": {
        "id": "VdQZqII8AMP5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Any questions?\n",
        "\n",
        "# Go be experimental!"
      ],
      "metadata": {
        "id": "DNhHssfSZRSO"
      }
    }
  ]
}